{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Our_Logistic_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBJRc0GwtGt+2evChSJbnh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calzateu/Modeling_and_Simulation_4/blob/main/Our_Logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5gFZ-WNWB_E"
      },
      "source": [
        "import pandas as pd\n",
        "from scipy import linalg as LA"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "sOc_oI2WKASW",
        "outputId": "0d51c384-c80c-436a-e572-4cd02ef7eeb8"
      },
      "source": [
        "data = pd.read_csv('Dataset-with-sentiments.csv')\n",
        "data.head(5)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Video ID</th>\n",
              "      <th>Comment ID</th>\n",
              "      <th>comments</th>\n",
              "      <th>Likes</th>\n",
              "      <th>polarity</th>\n",
              "      <th>pol_cat</th>\n",
              "      <th>stop_comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MwpMEbgC7DA</td>\n",
              "      <td>UgzSCaG-BeNW0LkW2px4AaABAg</td>\n",
              "      <td>that  s  beautiful</td>\n",
              "      <td>0</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>1</td>\n",
              "      <td>beautiful</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MwpMEbgC7DA</td>\n",
              "      <td>Ugw6J6RLt2VGaKMAz4h4AaABAg</td>\n",
              "      <td>imagine  being  able  to  listen  to  this  so...</td>\n",
              "      <td>176</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>1</td>\n",
              "      <td>imagine able listen song like first time ever ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MwpMEbgC7DA</td>\n",
              "      <td>UgwyFvqt1jZGR1l32sl4AaABAg</td>\n",
              "      <td>i've  memorized  this  song  more  than  math ...</td>\n",
              "      <td>4</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1</td>\n",
              "      <td>'ve memorized song math equations think song m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MwpMEbgC7DA</td>\n",
              "      <td>UgzzG2fg2Aq8HjJAQUB4AaABAg</td>\n",
              "      <td>every  time  i  listen  to  this  reminds  me ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>1</td>\n",
              "      <td>every time listen reminds tvd elena damon rain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MwpMEbgC7DA</td>\n",
              "      <td>Ugwx9ephHmJlIN8Sc614AaABAg</td>\n",
              "      <td>forever  my  cry  song</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>forever cry song</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Video ID  ...                                      stop_comments\n",
              "0  MwpMEbgC7DA  ...                                          beautiful\n",
              "1  MwpMEbgC7DA  ...  imagine able listen song like first time ever ...\n",
              "2  MwpMEbgC7DA  ...  've memorized song math equations think song m...\n",
              "3  MwpMEbgC7DA  ...  every time listen reminds tvd elena damon rain...\n",
              "4  MwpMEbgC7DA  ...                                   forever cry song\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAVjRxpEXhzs"
      },
      "source": [
        ""
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_gyN-auXh2F"
      },
      "source": [
        ""
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIdiQSNaXh62"
      },
      "source": [
        ""
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpRMqsFCXsmq",
        "outputId": "a5079f53-da3b-44c5-a856-df66fa20f9a4"
      },
      "source": [
        "sum(data['stop_comments'].isnull())\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2c0AVHVXsom"
      },
      "source": [
        "data = data[data['stop_comments'].notnull()]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X47G8LtaXsqa",
        "outputId": "5b31f7ea-31ae-47e7-8427-ae0f302b33e1"
      },
      "source": [
        "sum(data['stop_comments'].isnull())"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oJdYZIfkahB"
      },
      "source": [
        ""
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGATddlAkanR"
      },
      "source": [
        ""
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3j4DacKXssL",
        "outputId": "a985639a-f4ec-4bd1-8176-8a39217fcc7c"
      },
      "source": [
        "data['pol_cat']"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       1\n",
              "1       1\n",
              "2       1\n",
              "3       1\n",
              "4       0\n",
              "       ..\n",
              "1068    1\n",
              "1069    1\n",
              "1070    0\n",
              "1071    0\n",
              "1072    1\n",
              "Name: pol_cat, Length: 1073, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGN10wmnkbnJ"
      },
      "source": [
        "data[data['pol_cat']==-1] = 0"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVmBbynlkbt5",
        "outputId": "23e237a0-d1b7-4274-dc25-ef88fb9b8189"
      },
      "source": [
        "data['pol_cat']"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       1\n",
              "1       1\n",
              "2       1\n",
              "3       1\n",
              "4       0\n",
              "       ..\n",
              "1068    1\n",
              "1069    1\n",
              "1070    0\n",
              "1071    0\n",
              "1072    1\n",
              "Name: pol_cat, Length: 1073, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-vcKnLxlYzE"
      },
      "source": [
        ""
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JrH_VwMlY2C",
        "outputId": "d84d92d3-a806-4ed0-eed2-f045540dd606"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1073 entries, 0 to 1072\n",
            "Data columns (total 7 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   Video ID       1073 non-null   object \n",
            " 1   Comment ID     1073 non-null   object \n",
            " 2   comments       1073 non-null   object \n",
            " 3   Likes          1073 non-null   int64  \n",
            " 4   polarity       1073 non-null   float64\n",
            " 5   pol_cat        1073 non-null   int64  \n",
            " 6   stop_comments  1073 non-null   object \n",
            "dtypes: float64(1), int64(2), object(4)\n",
            "memory usage: 67.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HEC6o4mlwxg"
      },
      "source": [
        "data['stop_comments'] = data[['stop_comments']].astype(str)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AMmchcUXsue"
      },
      "source": [
        ""
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c7nj5ojXfk9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4zTUfJVXcV7"
      },
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(data['stop_comments'],data['pol_cat'],test_size = 0.2,random_state = 324)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaDH7-GalgVs"
      },
      "source": [
        ""
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w5FIpXfW_Y3"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
        "vect = CountVectorizer()\n",
        "tf_train = vect.fit_transform(X_train)\n",
        "tf_test = vect.transform(X_test)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCPw36U2YelA"
      },
      "source": [
        "tf_train = tf_train.toarray()\n",
        "tf_test = tf_test.toarray()"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVQ2Zy_H3HiX"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LogisticRegression as sklearnLogisticRegression\n",
        "\n",
        "class LogisticRegression:\n",
        "    \"\"\"\n",
        "            Purpose: To estimate Logistic regression parameters in Python.\n",
        "            Inputs:\n",
        "                alpha           : Is the optimisation learning rate.\n",
        "                maxIterations   : Maximum number of iterations for optimisation routine..\n",
        "                fitIntercept    : Include the intercept in the model fit.\n",
        "                verbose         : Display program information.\n",
        "                optimisation    : The optimisation routine to use. Options are:\n",
        "                                            gradientAscent\n",
        "                                            newton\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.01, maxIterations=100000, fitIntercept=True, verbose=False,optimisation=\"gradientAscent\"):\n",
        "        self.alpha = alpha\n",
        "        self.maxIterations = maxIterations  # Maximum number of times to run the optimisation.\n",
        "        self.numIterations = 0;             # Record the number of iterations performed.\n",
        "        self.hasConverged = False;          # This variable is used to terminate the iterations searching for optimum parameters.\n",
        "        self.fitIntercept = fitIntercept\n",
        "        self.verbose = verbose\n",
        "        self.costHistory = [];\n",
        "        self.tolerance = tol=1e-7; # convergence tolerance;\n",
        "        self.theta=[];\n",
        "        self.optimisation = optimisation;\n",
        "\n",
        "    def __add_intercept(X):\n",
        "        X = X.reshape(len(X), -1)\n",
        "        intercept = np.ones((X.shape[0], 1))\n",
        "        return np.concatenate((intercept, X), axis=1)\n",
        "\n",
        "    def __sigmoid(z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "    def __cost(self, X, y, theta):\n",
        "        # Purpose: Logistic regression log cost function.\n",
        "        z = np.dot(X, theta)\n",
        "        p = LogisticRegression.__sigmoid(z)\n",
        "        return (-y * np.log(p) - (1 - y) * np.log(1 - p)).mean()\n",
        "\n",
        "    def gradientAscent(X,y,theta,alpha):\n",
        "        z = np.dot(X, theta)\n",
        "        p = LogisticRegression.__sigmoid(z)\n",
        "        gradient = np.dot(X.T, (p - y)) / y.size;  # 1st derivative of log likelihood wrt parameters.\n",
        "        theta -= alpha * gradient  # Update the parameters.\n",
        "\n",
        "        return theta;\n",
        "\n",
        "    def newton(X,y,theta,useRegulisation=False,regulisationParameter=0):\n",
        "        \"\"\" Newton optimisation method.\"\"\"\n",
        "        #print(\"theta\", theta)\n",
        "        z = np.dot(X, theta)\n",
        "        #print('z', z)\n",
        "        p = LogisticRegression.__sigmoid(z)\n",
        "        #print('p', p)\n",
        "        W = np.diag(p * (1 - p))\n",
        "        #print('W', W)\n",
        "        hessian = X.T.dot(W).dot(X);\n",
        "\n",
        "        eig_vals, eig_vects = LA.eig(hessian)\n",
        "\n",
        "        #val = -min(eig_vals)\n",
        "        val = 2\n",
        "\n",
        "        a = np.zeros((len(hessian), len(hessian)),int) #Inicializo una matriz\n",
        "        np.fill_diagonal(a, np.linalg.norm(val)) # Relleno la diagonal con un valor especifico\n",
        "\n",
        "        print(val)\n",
        "        print(a)\n",
        "        hessian = hessian + a\n",
        "        print(hessian)\n",
        "\n",
        "\n",
        "        gradient = np.dot(X.T, (y-p));  # 1st derivative of log likelihood wrt parameters.\n",
        "\n",
        "        \n",
        "        try:\n",
        "            if useRegulisation:\n",
        "                step = np.dot(np.linalg.inv(hessian + regulisationParameter * np.eye(theta)), grad)\n",
        "            else:\n",
        "                #print('Pasó')\n",
        "                step = np.dot(np.linalg.inv(hessian), gradient)\n",
        "                #print('Funcionó')\n",
        "        except np.linalg.LinAlgError:\n",
        "            step=0;\n",
        "        \n",
        "\n",
        "        #print(hessian.shape)\n",
        "        #step = np.dot(np.linalg.inv(hessian), gradient)\n",
        "\n",
        "        ## update the weights\n",
        "        theta = theta + step\n",
        "\n",
        "\n",
        "        return theta;\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if self.fitIntercept:\n",
        "            X = LogisticRegression.__add_intercept(X=X)\n",
        "\n",
        "\n",
        "        self.theta = np.zeros(X.shape[1]);  # Initialise weights.\n",
        "        alpha=self.alpha;\n",
        "\n",
        "        cost = self.__cost(X, y, self.theta);  # Calculate the cost.\n",
        "        self.costHistory.append(cost);  # Record the intitial cost for plotting.\n",
        "\n",
        "\n",
        "        iterCount=0;\n",
        "\n",
        "        while not self.hasConverged:\n",
        "            # Perform the optimisation many times to reduce the cost by improving the parameters.\n",
        "            iterCount+=1;                       # Counter for the number of optimisation iterations.\n",
        "\n",
        "            # Depending on the optimisation approach. Calculate the coeffient update step differently.\n",
        "            if self.optimisation==\"gradientAscent\":\n",
        "                theta = LogisticRegression.gradientAscent(X,y,self.theta,self.alpha)\n",
        "                iterDisplayVerbose = 10000;\n",
        "            elif self.optimisation==\"newton\":\n",
        "                theta = LogisticRegression.newton(X, y, self.theta)\n",
        "                iterDisplayVerbose = 1;             # The number of iteration steps is significantly less than Gradient Ascent.\n",
        "            else:\n",
        "                assert \"unknown optimisation routine.\"\n",
        "                return;\n",
        "\n",
        "            cost = self.__cost(X,y, theta);     # Calculate the cost.\n",
        "\n",
        "\n",
        "            if iterCount>1:\n",
        "            # Only check to terminate optimisation after performing the second optimisation calculation.\n",
        "                hasConverged = self.__checkConvergence(self.costHistory[-1], cost, self.tolerance,iterCount);  # Check if should terminate iteration updates as convergence tolerance has been reached.\n",
        "\n",
        "                if hasConverged.hasConverged==True:\n",
        "                    print(\"Iteration #:  {:>7,.0f}.  Cost: {:>+7.4f}.\".format(iterCount, cost));\n",
        "                    print(\"Finished because {}. Using {} optimisation method.\".format(hasConverged.reason, self.optimisation));\n",
        "                    self.numIterations = iterCount;\n",
        "                    self.hasConverged == True;\n",
        "\n",
        "                if (self.verbose == True and iterCount % iterDisplayVerbose == 0) and  hasConverged.hasConverged==False:\n",
        "                    # Print out the log output.\n",
        "                    print(\"Iteration #:  {:>7,.0f}.  Cost: {:>+7.4f}\".format(iterCount, cost));\n",
        "\n",
        "\n",
        "            if iterCount < 2:\n",
        "\n",
        "                if (self.verbose == True and iterCount % iterDisplayVerbose == 0):\n",
        "                    # Print out the log output.\n",
        "                    print(\"Iteration #:  {:>7,.0f}.  Cost: {:>+7.4f}\".format(iterCount, cost));\n",
        "\n",
        "                self.theta = theta;\n",
        "                self.costHistory.append(cost);  # Record the cost for plotting.\n",
        "            else:\n",
        "                if not (((self.optimisation == \"newton\") and (cost > self.costHistory[-1])) or (np.isnan(cost))) or iterCount<1:\n",
        "                    # The Newton method on the last step can give coefficients well off and a worse cost as close to the singularity.\n",
        "                    # Because of this. Not recording the last theta and cost found in this case and using the previous one.\n",
        "                    self.theta = theta;\n",
        "                    self.costHistory.append(cost);  # Record the cost for plotting.\n",
        "\n",
        "\n",
        "    def __checkConvergence(self,previousCost, cost, tolerance, iterCount):\n",
        "        ''' Purpose: Checks if coefficients have converged.\n",
        "            Returns True if they have converged, False otherwise.'''\n",
        "        costChange = np.abs(previousCost - cost)\n",
        "\n",
        "        self.reason=\"\";\n",
        "        self.hasConverged=False;\n",
        "        if (np.any(costChange < tolerance)):\n",
        "            self.reason = \"cost function tolerance reached\";\n",
        "            self.hasConverged = True;\n",
        "\n",
        "        # If havn't reached thresholds, perform more iterations (keep training).\n",
        "        if (iterCount > self.maxIterations):\n",
        "            self.reason=\"maximum iterations reached\"\n",
        "            self.hasConverged = True;\n",
        "\n",
        "\n",
        "        if (self.optimisation == \"newton\"):\n",
        "            if (np.isnan(cost)):\n",
        "                # The Newton method on the last step can give coefficients well off and a worse cost as close to the singularity.\n",
        "                # Because of this. Not recording the last theta and cost found in this case and using the previous one.\n",
        "                self.reason=\"singular Hessian\"\n",
        "                self.hasConverged = True;\n",
        "            elif (cost > previousCost):\n",
        "                # The Newton method on the last step can give coefficients well off and a worse cost as close to the singularity.\n",
        "                # Because of this. Not recording the last theta and cost found in this case and using the previous one.\n",
        "                self.reason=\"cost function worsoning as close to solution\"\n",
        "                self.hasConverged = True;\n",
        "\n",
        "\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def predict_prob(X,theta,fitIntercept=True):\n",
        "        if fitIntercept:\n",
        "            X = LogisticRegression.__add_intercept(X=X)\n",
        "\n",
        "        print(len(X), len(theta))\n",
        "        return LogisticRegression.__sigmoid(np.dot(X, theta))\n",
        "\n",
        "    def predict(self, X):\n",
        "        #return LogisticRegression.predict_prob((X).round(),self.theta,self.fitIntercept)\n",
        "        prob = LogisticRegression.predict_prob((X).round(),self.theta,self.fitIntercept)\n",
        "        if (prob > 0.5):\n",
        "          return 1\n",
        "        return 0\n",
        "\n",
        "    def formattedOutput(objLogisticRegression):\n",
        "        \"\"\"\n",
        "        Purpose: To produce readable output summary of the results.\n",
        "            Input:\n",
        "                LogisticRegression  : A class instance containing the fitted information.\n",
        "        \"\"\"\n",
        "        optimisationMethod=objLogisticRegression.optimisation;\n",
        "        theta=objLogisticRegression.theta;\n",
        "        inititialCost=objLogisticRegression.costHistory[0];\n",
        "        finalCost=objLogisticRegression.costHistory[-1];\n",
        "        numIterations=objLogisticRegression.numIterations;\n",
        "\n",
        "\n",
        "        dash = '=' * 80; #chr(10000)*50\n",
        "        print(dash)\n",
        "        print(\"LOGISTIC REGRESSION USING {0} TERRMINATION RESULTS\".format(optimisationMethod.upper()))\n",
        "        print(dash)\n",
        "        print(\"Initial Weights were:    {:>12.1f}, {:>2.1f}, {:>2.1f}.\".format(0, 0, 0))\n",
        "        print(\"   With initial cost:    {:>+12.6f}.\".format(inititialCost))\n",
        "        print(\"        # Iterations:    {:>+12,.0f}.    \".format(numIterations))\n",
        "        print(\"       Final weights:    theta0:{:>+0.2f}, theta1:{:>+3.2f}, theta02:{:>+3.3f}.\".format(\n",
        "            theta[0], theta[1], theta[2]))#print(\"       Final weights:    \\u03F4\\u2080:{:>+0.2f}, \\u03F4\\u2081:{:>+3.2f}, \\u03F4\\u2082:{:>+3.3f}.\".format(theta[0], theta[1], theta[2]))\n",
        "        print(\"          Final cost:    {:>+12.6f}.\".format(finalCost))\n",
        "        print(dash)\n"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZqJbEnDmnIr"
      },
      "source": [
        ""
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMwLPu4JSQwQ",
        "outputId": "9ec3cc76-9205-4ac3-d4b6-765f2dbbc86d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf_train.shape"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(365, 1770)"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZpEcvAmSTAN",
        "outputId": "16e586cf-88cd-41c7-f721-4db2be4d0b55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf_test.shape"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(215, 1770)"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0sHWET9SQ1b"
      },
      "source": [
        ""
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg48ld4vmnKv"
      },
      "source": [
        "tf_train = tf_train"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRiyLOKHmnl5",
        "outputId": "d833e5fe-4f7a-4fc6-fcd5-d2746edf47d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "corr_matrix = np.corrcoef(tf_train)\n",
        "sumas = sum(corr_matrix >= 0.5)\n",
        "indices = sumas > 1\n",
        "tf_train = tf_train[~indices]\n",
        "\n",
        "y_train = y_train[~indices]\n",
        "\n",
        "tf_train.shape, y_train.shape"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((365, 1770), (365,))"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWU0Uaa9I8G2"
      },
      "source": [
        "tf_train = tf_train"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-ESm6ZwI8JN",
        "outputId": "2872d356-81ca-4af4-f7af-811a0efb1d78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf_train.shape"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(365, 1770)"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dhmmn1WrSljK",
        "outputId": "a1016cdc-02a2-44fb-e3aa-cfe95bb142a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf_test.shape"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(215, 1770)"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZpNrztFmn1u"
      },
      "source": [
        ""
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNw26hiOWVP4",
        "outputId": "1a408287-c36b-449d-9935-1a2d37c0b461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "'''def run():\n",
        "\n",
        "    #\n",
        "    ## Load Data:\n",
        "    #iris = datasets.load_iris()\n",
        "    #X = iris.data[:, :2]\n",
        "    #y = (iris.target != 0) * 1\n",
        "    #\n",
        "\n",
        "    X = tf_train\n",
        "    y = y_train\n",
        "\n",
        "\n",
        "    # Run the model:\n",
        "    objLogisticRegression = LogisticRegression(alpha=0.1, maxIterations=10,fitIntercept=True, verbose=True,optimisation=\"newton\");   # Initialise the regression.\n",
        "    objLogisticRegression.fit(X, y);                                 # Fit the regression.\n",
        "\n",
        "    # Show the output:\n",
        "    LogisticRegression.formattedOutput(objLogisticRegression=objLogisticRegression);        # Show the formatted results.\n",
        "\n",
        "\n",
        "    #\n",
        "    # Run the model:\n",
        "    #objLogisticRegression = LogisticRegression(alpha=0.1, maxIterations=100000,fitIntercept=True, verbose=True,optimisation=\"gradientAscent\");   # Initialise the regression.\n",
        "    #objLogisticRegression.fit(X, y);                                 # Fit the regression.\n",
        "\n",
        "    # Show the output:\n",
        "    #LogisticRegression.formattedOutput(objLogisticRegression=objLogisticRegression);        # Show the formatted results.\n",
        "    #\n",
        "\n",
        "    # sklearn's Logistic Regression.\n",
        "    model = sklearnLogisticRegression(C=1e8).fit(X, y)\n",
        "    dash = '=' * 80;  # '=' * 80;\n",
        "    print(dash)\n",
        "    print(\"LOGISTIC REGRESSION USING SKLEARN TERMINATION RESULTS\")\n",
        "    print(\"Final weights:    theta0:{:>+0.2f}, theta1:{:>+0.2f}, theta2:{:>+0.2f}.\".format(model.intercept_[0], model.coef_[0][0], model.coef_[0][1]))\n",
        "    print(dash)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Finished\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()'''"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'def run():\\n\\n    #\\n    ## Load Data:\\n    #iris = datasets.load_iris()\\n    #X = iris.data[:, :2]\\n    #y = (iris.target != 0) * 1\\n    #\\n\\n    X = tf_train\\n    y = y_train\\n\\n\\n    # Run the model:\\n    objLogisticRegression = LogisticRegression(alpha=0.1, maxIterations=10,fitIntercept=True, verbose=True,optimisation=\"newton\");   # Initialise the regression.\\n    objLogisticRegression.fit(X, y);                                 # Fit the regression.\\n\\n    # Show the output:\\n    LogisticRegression.formattedOutput(objLogisticRegression=objLogisticRegression);        # Show the formatted results.\\n\\n\\n    #\\n    # Run the model:\\n    #objLogisticRegression = LogisticRegression(alpha=0.1, maxIterations=100000,fitIntercept=True, verbose=True,optimisation=\"gradientAscent\");   # Initialise the regression.\\n    #objLogisticRegression.fit(X, y);                                 # Fit the regression.\\n\\n    # Show the output:\\n    #LogisticRegression.formattedOutput(objLogisticRegression=objLogisticRegression);        # Show the formatted results.\\n    #\\n\\n    # sklearn\\'s Logistic Regression.\\n    model = sklearnLogisticRegression(C=1e8).fit(X, y)\\n    dash = \\'=\\' * 80;  # \\'=\\' * 80;\\n    print(dash)\\n    print(\"LOGISTIC REGRESSION USING SKLEARN TERMINATION RESULTS\")\\n    print(\"Final weights:    theta0:{:>+0.2f}, theta1:{:>+0.2f}, theta2:{:>+0.2f}.\".format(model.intercept_[0], model.coef_[0][0], model.coef_[0][1]))\\n    print(dash)\\n\\n\\n\\n    print(\"Finished\")\\n\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    run()'"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAcE0Y2al_vd"
      },
      "source": [
        ""
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6S9hQAXlqMu",
        "outputId": "120e8da5-4c98-4276-8023-dd9fb52c16ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X = tf_train\n",
        "y = y_train\n",
        "\n",
        "\n",
        "# Run the model:\n",
        "objLogisticRegression = LogisticRegression(alpha=0.1, maxIterations=10,fitIntercept=False, verbose=True,optimisation=\"newton\");   # Initialise the regression.\n",
        "objLogisticRegression.fit(X, y);                                 # Fit the regression.\n",
        "\n",
        "# Show the output:\n",
        "LogisticRegression.formattedOutput(objLogisticRegression=objLogisticRegression);        # Show the formatted results."
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 2 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 2]]\n",
            "[[2.   0.   0.   ... 0.   0.   0.  ]\n",
            " [0.   2.5  0.   ... 0.   0.   0.  ]\n",
            " [0.   0.   2.25 ... 0.   0.   0.  ]\n",
            " ...\n",
            " [0.   0.   0.   ... 2.5  0.   0.  ]\n",
            " [0.   0.   0.   ... 0.   2.   0.  ]\n",
            " [0.   0.   0.   ... 0.   0.   2.  ]]\n",
            "Iteration #:        1.  Cost: +0.3449\n",
            "2\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 2 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 2]]\n",
            "[[2.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         2.3083168  0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         2.24250021 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 2.26007771 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         2.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         2.        ]]\n",
            "Iteration #:        2.  Cost: +0.2278\n",
            "2\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 2 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 2]]\n",
            "[[2.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         2.18688697 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         2.21986413 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 2.12810893 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         2.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         2.        ]]\n",
            "Iteration #:        3.  Cost: +0.1673\n",
            "2\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 2 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 2]]\n",
            "[[2.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         2.12196948 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         2.19208879 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 2.06970662 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         2.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         2.        ]]\n",
            "Iteration #:        4.  Cost: +0.1312\n",
            "2\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 2 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 2]]\n",
            "[[2.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         2.0865662  0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         2.16580515 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 2.04339383 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         2.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         2.        ]]\n",
            "Iteration #:        5.  Cost: +0.1074\n",
            "2\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 2 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 2]]\n",
            "[[2.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         2.06604659 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         2.14332455 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 2.0303545  0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         2.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         2.        ]]\n",
            "Iteration #:        6.  Cost: +0.0906\n",
            "2\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 2 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 2]]\n",
            "[[2.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         2.05312021 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         2.12478989 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 2.02298286 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         2.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         2.        ]]\n",
            "Iteration #:        7.  Cost: +0.0782\n",
            "2\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 2 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 2]]\n",
            "[[2.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         2.04432599 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         2.10965885 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 2.01831416 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         2.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         2.        ]]\n",
            "Iteration #:        8.  Cost: +0.0686\n",
            "2\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 2 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 2]]\n",
            "[[2.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         2.03797947 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         2.09728138 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 2.01511517 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         2.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         2.        ]]\n",
            "Iteration #:        9.  Cost: +0.0611\n",
            "2\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 2 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 2]]\n",
            "[[2.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         2.03319175 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         2.08708185 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 2.01280065 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         2.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         2.        ]]\n",
            "Iteration #:       10.  Cost: +0.0550\n",
            "2\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 2 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 2]]\n",
            "[[2.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         2.02945595 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         2.07859701 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 2.01105826 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         2.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         2.        ]]\n",
            "Iteration #:       11.  Cost: +0.0499.\n",
            "Finished because maximum iterations reached. Using newton optimisation method.\n",
            "================================================================================\n",
            "LOGISTIC REGRESSION USING NEWTON TERRMINATION RESULTS\n",
            "================================================================================\n",
            "Initial Weights were:             0.0, 0.0, 0.0.\n",
            "   With initial cost:       +0.693147.\n",
            "        # Iterations:             +11.    \n",
            "       Final weights:    theta0:+0.00, theta1:-0.40, theta02:+1.001.\n",
            "          Final cost:       +0.049924.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f32eSgJdPQqo"
      },
      "source": [
        "#corr_matrix = np.corrcoef(tf_test)\n",
        "#sumas = sum(corr_matrix >= 0.5)\n",
        "#indices = sumas > 1\n",
        "#tf_test = tf_test[~indices]\n",
        "\n",
        "#y_test = y_ttest[~indices]\n",
        "\n",
        "#tf_test.shape, y_test.shape"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFsP_V07R03p",
        "outputId": "b7f33f56-8645-4c9c-82f1-2d6d66cfa193",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf_test.shape"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(215, 1770)"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfajBx8bSC0x",
        "outputId": "f9aa6067-532c-404b-edfa-9d689f087258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf_train.shape"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(365, 1770)"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CghAhytpjnIS",
        "outputId": "06a0df7d-1b8a-4dcc-8339-5d7e7361193c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "correctos = 0\n",
        "\n",
        "y_train = list(y_train)\n",
        "\n",
        "for i in range(len(tf_train)):\n",
        "  prediccion = objLogisticRegression.predict(tf_train[i])\n",
        "  #print(prediccion)\n",
        "  if (prediccion == y_train[i]):\n",
        "    correctos += 1\n",
        "\n",
        "print(f'Accuraccy: {correctos/len(tf_train)}')"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "Accuraccy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK3eyI-5NlRw",
        "outputId": "df0738cc-9214-40ee-9fae-beb1bd8d44ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "correctos = 0\n",
        "\n",
        "y_test = list(y_test)\n",
        "\n",
        "for i in range(len(tf_test)):\n",
        "  prediccion = objLogisticRegression.predict(tf_test[i])\n",
        "  #print(prediccion)\n",
        "  if (prediccion == y_test[i]):\n",
        "    correctos += 1\n",
        "\n",
        "print(f'Accuraccy: {correctos/len(tf_test)}')"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "1770 1770\n",
            "Accuraccy: 0.7488372093023256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF6F8ExRikdK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KySCOjfdeLQY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}